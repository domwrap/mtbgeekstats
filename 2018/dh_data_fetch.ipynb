{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI MTB DH Data Retrieval\n",
    "\n",
    "## Setup\n",
    "#### Import Libraries\n",
    "\n",
    "If you do not have these libraries available, you should install them using `pip`\n",
    "\n",
    "```\n",
    "pip install requests\n",
    "pip install bs4\n",
    "pip install pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widen display area to prevent column wrapping, and always show all columns for debug "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 2000)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config\n",
    "\n",
    "Which race data are we collecting?\n",
    "\n",
    "1. Losinj\n",
    "1. Fort William\n",
    "1. Leogang\n",
    "1. Val di Sole\n",
    "1. Vallnord\n",
    "1. Mont-Sainte-Anne\n",
    "1. La Bresse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "race = 1\n",
    "gender = 'f'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Sources\n",
    "\n",
    "The UCI Live Timing API contains a lot of data points, but not all the ones we want (speed being the main one missing), and not even all the ones they include on their own PDF which is frustrating.\n",
    "\n",
    "Similarly, Roots & Rain also has a lot of the data points, but again not all of them; most notably it's missing timing splits 4 and 5.\n",
    "\n",
    "Therefore we need to pull from both sources and combine the sets.\n",
    "\n",
    "Here we specify the URLs for both sources from which we will extract our data. The UCI API URL can be found by loading the Live Timing page then using your browser's inspector on the Network tab (in Chrome at least) to see the data feed. As the UCI seems to be using a Single Page Application (SPA) here, it's not straight forward to extract this link automagically.\n",
    "\n",
    "*Links will be added as data sources become available.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "races = [\n",
    "    [\n",
    "        'losinj',\n",
    "         'http://prod.chronorace.be/api/results/uci/dh/race/20180421_dh/3',\n",
    "         'https://www.rootsandrain.com/race5897/2018-apr-22-mercedes-benz-uci-world-cup-1-losinj/results/filters/m/',\n",
    "         'http://prod.chronorace.be/api/results/uci/dh/race/20180421_dh/6',\n",
    "         'https://www.rootsandrain.com/race5897/2018-apr-22-mercedes-benz-uci-world-cup-1-losinj/results/filters/f/'\n",
    "    ]\n",
    "    , [ 'fortbill', '', '' ]\n",
    "    , [ 'leogang', '', '' ]\n",
    "    , [ 'valdisole', '', '' ]\n",
    "    , [ 'vallnord', '', '' ]\n",
    "    , [ 'msa', '', '' ]\n",
    "    , [ 'labresse', '', '' ]\n",
    "]\n",
    "key = 0 if gender == 'm' else 2\n",
    "raceName = races[race-1][0]\n",
    "urlUci = races[race-1][key+1]\n",
    "urlRoots = races[race-1][key+2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCI API\n",
    "### Load Data\n",
    "\n",
    "These two lines make the actual request to the server, and then converts the JSON string response in to a usable list format (deserialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get( urlUci )\n",
    "d = r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API returns with three main sections:\n",
    "\n",
    "1. `Last Finisher`\n",
    " - Racers in order of start time\n",
    "2. `Results`\n",
    " - Racers in finishing rank order\n",
    "3. `Riders`\n",
    " - Personal details on all racers\n",
    " \n",
    "Each contains many data points. To see all the contained data, you can un-comment and execute any of the lines in the next section to explore more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display( d )\n",
    "# display( d['Results'][7] )\n",
    "# display( d['Riders']['1001'] )\n",
    "# display( d['Results'][61] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Data\n",
    "\n",
    "Here we iterate over the `Results` sub-set of data to extract the information we care about: basically those that finished the race, some identifying info, and their splits.\n",
    "\n",
    "If you looked at detail of the returned data set in the last step you might have noticed the rider's name is not stored next to their result, riders are only identified by a reference number. To facilitate our analysis later on it is useful to import each rider's name at this stage by cross-referencing the `Riders` sub-set.\n",
    "\n",
    "First we find out the last man to drop-in's start number so we can use that to add a reverse order column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastStart = d['Riders'][list(d['Riders'].keys())[-1]]['StartOrder']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with an empty list `lst` and in each loop iteration add an entry (actually a dict) to that list for each rider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = len(d['Results'][0]['Times'] )\n",
    "lst = []\n",
    "for idx, row in enumerate( d['Results'] ):\n",
    "    fin = \"Finished\" == row['Status']\n",
    "    res = {\n",
    "        'rank': row['Position'] if fin else idx+1,\n",
    "        'name': d['Riders'][str(row['RaceNr'])]['PrintName'],\n",
    "        'id': row['RaceNr'],\n",
    "        'uci': d['Riders'][str(row['RaceNr'])]['UciRiderId'],\n",
    "        'bib': d['Riders'][str(row['RaceNr'])]['RaceNr'],\n",
    "        'status': row['Status'],\n",
    "        'speed': np.nan,\n",
    "        'start': d['Riders'][str(row['RaceNr'])]['StartOrder'],\n",
    "        'start_rev': lastStart - d['Riders'][str(row['RaceNr'])]['StartOrder'] +1\n",
    "    }\n",
    "\n",
    "    # Add all splits to result set\n",
    "    for split in range( 0, splits ):\n",
    "        res['split' + str(split+1)] = row['Times'][split]['RaceTime']/1000 if fin else np.nan\n",
    "\n",
    "    # Append result set to list\n",
    "    lst.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line loads the completed list in to a Pandas dataframe so that we can easily write it out to CSV later on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame( lst )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expand Dataset\n",
    "\n",
    "Calculate and add all the extra columns we need for split and sector differences and their rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range( 1, splits+1 ):\n",
    "    split = 'split' + str(i)\n",
    "    sector = split + '_sector'\n",
    "    df[split + '_rank'] = df[split].rank(method='dense')\n",
    "    df[split + '_vs_best'] = (df[split] - df[split].min())\n",
    "    df[split + '_vs_winner'] = (df[split] - df[split][0])\n",
    "    \n",
    "    if i > 1:\n",
    "        df[split + '_sector'] = df[split] - df['split' + str(i-1)]\n",
    "        df[split + '_sector_rank'] = df[sector].rank(method='dense')\n",
    "        df[split + '_sector_vs_best'] = (df[sector] - df[sector].min())\n",
    "        df[split + '_sector_vs_winner'] = (df[sector] - df[sector][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a peek at our data at this point to make sure it looks how we expect.\n",
    "\n",
    "At this point the `speed` column is NaN (Not a Number) for all racers. This will be filled in below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display( df.head(10) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rider Data\n",
    "\n",
    "Saving the personal information about each racer is much easier as we can just export the entire `Riders` dataset. However, the rows and columns are the wrong way round so the `.T` command *transposes* the information, meaning it basically flips the axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame( d['Riders'] )\n",
    "df2 = df2.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can glimpse the first few rows of our `DataFrame` and can check the data looks as we expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display( df2.head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roots and Rain\n",
    "### Load Data\n",
    "\n",
    "Similar to the UCI api, we make a request to the server with the previously declared `urlRoots` variable. This time however we simply load the content of the response as text which is actually the HTML code of the web page. We do not do have a nice JSON API to read which means we will not deserialize.\n",
    "\n",
    "Next we invoke a utility called `BeautifulSoup` to help us extract the data from this messy HTML code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post( urlRoots )\n",
    "c = r.content\n",
    "soup = BeautifulSoup( c, \"html.parser\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Data\n",
    "\n",
    "If you look at the Roots and Rain page you'll see it listed in a tabular format. What we do here is find all the rows of that table so we can extract the information we need.\n",
    "\n",
    "Specifically we are looking for instances of `tr` (table row), with a class that *begins with* `c-` as this is a common denomenator I discovered when looking through the code with the browser inspector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = soup.find_all( \"tr\", class_=lambda x: x and 'c-' in x )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the UCI data set, here we will iterate over each row in our data set--basically each table row from the web page--and extract the bits we need.\n",
    "\n",
    "Racer speed is the metric we're interested in, but in order to match that to our existing data set we need a corresponding identifier so we also extract the racer licence number as that exists in both sets and we can match them together: it is the *intersect* between both sets of data.\n",
    "\n",
    "To summarise:\n",
    "1. Extract licence number and corresponding speed\n",
    "2. Import speed to existing DataFrame matching racers by licence\n",
    "\n",
    "The `if` condition in the middle will exit this block of code once we hit the end of the Elite finishers, seeing as that's all we have in our existing data set so can't match anyone else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows:\n",
    "    cells = row.find_all( \"td\" )\n",
    "\n",
    "    speed = float(cells[7].text[:5])\n",
    "    licence = cells[4].text\n",
    "    bib = int( cells[1].text )\n",
    "    pos = cells[0].text[8:]\n",
    "    if \"\" == pos: break\n",
    "\n",
    "    # Match rider by UCI licence if present, otherwise fallback to bib\n",
    "    if len(df2.loc[df2['UciRiderId'] == licence].index.values ):\n",
    "        rid = int(df2.loc[df2['UciRiderId'] == licence].index.values[0])\n",
    "    else:\n",
    "        rid = int( df2.loc[df2['RaceNr'] == bib].index.values[0] )\n",
    "\n",
    "    # Add speed, and other associated metrics\n",
    "    df.loc[df['id'] == rid, 'speed'] = speed\n",
    "    df.loc[df['id'] == rid, 'speed_ms'] = float(speed)*(1000/60/60)\n",
    "    df.loc[df['id'] == rid, 'speed_ms_vs_best'] = df['speed_ms'].max() - df.speed_ms\n",
    "    df['speed_rank'] = df.speed.rank(method='dense', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can take another look at how our data is looking, with the `speed` column now containing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display( df.head() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Export\n",
    "\n",
    "All that's left is to save our data to CSV files so we can quickly import it again for analysis and visualization without making constant requests to the online servers. This not only reduces load on the services providing the data, but also allows us to work on our analysis \"offline\", moreover giving us a local copy in case the results are ever taken down. It's also much quicker to load data this way than constantly hitting online servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = 'dh'\n",
    "filePrefix = event + '_' + str(race) + '_' + raceName + '_' + gender\n",
    "df.to_csv( filePrefix + '.results.csv' )\n",
    "df2.to_csv( filePrefix + '.racers.csv' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Credits\n",
    "\n",
    "### Author: Dominic Wrapson\n",
    "\n",
    "\n",
    "> **@domwrap**\n",
    "<br>\n",
    "<img src=\"https://png.icons8.com/material/24/000000/github-2.png\">\n",
    "<img src=\"https://png.icons8.com/material/24/000000/stackoverflow.png\">\n",
    "<img src=\"https://png.icons8.com/material/24/000000/linkedin.png\">\n",
    "<img src=\"https://png.icons8.com/material/24/000000/windows8.png\">\n",
    "<img src=\"https://png.icons8.com/ios-glyphs/24/000000/instagram-new.png\">\n",
    "<img src=\"https://png.icons8.com/material/24/000000/twitter.png\">\n",
    "<a href=\"https://medium.com/@domwrap\"><img src=\"https://png.icons8.com/material/24/000000/medium-logo.png\"></a>\n",
    ">\n",
    "> <img src=\"https://png.icons8.com/material/24/000000/home.png\"> http://domwrap.me\n",
    ">\n",
    "><img src=\"https://png.icons8.com/material/24/000000/cycling-mountain-bike.png\"> [Hwulex](https://www.pinkbike.com/u/Hwulex/)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Special Thanks\n",
    "\n",
    "Mark Shilton for the inspiration\n",
    "- http://lookatthestats.blogspot.ca\n",
    "- https://plus.google.com/+MarkShilton\n",
    "- https://dirtmountainbike.com/author/mrgeekstats\n",
    "\n",
    "\n",
    "<a href=\"https://icons8.com\">Icon pack by Icons8</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
